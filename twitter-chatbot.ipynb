{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b27e24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9df92808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20_000\n",
    "NUM_SAMPLES = 10_000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02f4be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 8490\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "input_texts = []  # sentence in original language\n",
    "target_texts = []  # sentence in target language\n",
    "target_text_inputs = []  #  sentence in target language offset by 1\n",
    "t = 0\n",
    "for line in open(\"./twitter_tab_format.txt\"):\n",
    "    # only keep a limited number of samples\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "    line = line.rstrip()\n",
    "    # input and targets are separeted by tab\n",
    "    if \"\\t\" not in line:\n",
    "        continue\n",
    "\n",
    "    # split up the input and translation\n",
    "    input_text, translation = line.split(\"\\t\")[:2]\n",
    "\n",
    "    # make the target input and output\n",
    "    # recall we'll be using teacher forcing\n",
    "    target_text = translation + \" <eos>\"\n",
    "    target_text_input = \"<sos> \" + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_text_inputs.append(target_text_input)\n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4651a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9908 unique input tokens\n"
     ]
    }
   ],
   "source": [
    "#  tokenize inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "\n",
    "# get word -> integer mapping\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print(\"Found %s unique input tokens\" % len(word2idx_inputs))\n",
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85d74c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13053 unique output tokens\n"
     ]
    }
   ],
   "source": [
    "# tokenize outputs\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters=\"\")\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_text_inputs)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_text_inputs)\n",
    "\n",
    "# get word -> integer mapping for outputs\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print(\"Found %s unique output tokens\" % len(word2idx_outputs))\n",
    "\n",
    "\n",
    "# store number of output wors for later\n",
    "# remember to add 1 since indexing start at 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf0a3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_data.shape: (8490, 34)\n",
      "encoder_data[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  227   27 5070   51   65\n",
      "    4   34   77   16  278  196]\n",
      "decoder_data.shape: (8490, 33)\n",
      "decoder_data[0]: [   2   18  120  479  380    8   27   82   55 5453   55    3 1441   15\n",
      " 5454   12   67 2243   26   27  166    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(\n",
    "    target_sequences_inputs, maxlen=max_len_target, padding=\"post\"\n",
    ")\n",
    "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
    "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f883a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors ...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print(\"loading word vectors ...\")\n",
    "word2vec_path = '../Lazyprogrammer/large_files/glove.6B/glove.6B.%sd.txt'\n",
    "word2vec = {}\n",
    "with open(\n",
    "    os.path.join(word2vec_path % EMBEDDING_DIM)\n",
    ") as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2]\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.array(values[1:], dtype=\"float32\")\n",
    "        word2vec[word] = vec\n",
    "    print(\"Found %s word vectors.\" % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "909b1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print(\"Filling pre-trained embeddings...\")\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91f26677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data set\n",
    "class TwitterDatset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.encoder_in = torch.from_numpy(encoder_inputs).int()\n",
    "        self.decoder_in = torch.from_numpy(decoder_inputs).int()\n",
    "        self.decoder_out = torch.from_numpy(decoder_targets).float()\n",
    "        #self.decoder_out = torch.from_numpy(decoder_targets_one_hot).float()\n",
    "    def __len__(self):\n",
    "        return len(encoder_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        encoder_in = self.encoder_in[idx]\n",
    "        decoder_in = self.decoder_in[idx]\n",
    "        decoder_out = self.decoder_out[idx]\n",
    "\n",
    "    \n",
    "        return encoder_in, decoder_in, decoder_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b00baa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset\n",
    "twitter_dataset = TwitterDatset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "911971a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=twitter_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386f5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an embedding layer\n",
    "# freeze the layer\n",
    "embedding_layer = nn.Embedding(num_words, EMBEDDING_DIM)  # vocab size  # embedding dim\n",
    "embedding_layer.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "embedding_layer.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de69bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "# EMBEDDING_DIM = 100\n",
    "# LATENT_DIM = 256\n",
    "# T encoder = 4\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size # LATENT_DIM\n",
    "        self.num_layers = num_layers # 1 or 2\n",
    "\n",
    "        self.embedding = embedding_layer # vocab size x EMBEDDING_DIM\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)#, dropout=p) # -> T x N x 2*hidden\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (N, T encoder) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x)) # (T encoder, N, EMBEDDING_DIM)\n",
    "        # embedding shape: # (T encoder, N, EMBEDDING_DIM)\n",
    "\n",
    "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
    "        #  encoder_states shape:  (T encoder *num_layers , N, 2*hidden_size) \n",
    "        # hidden, cell : (2*num_layers, N, hidden_size) bidirectional = True\n",
    "        \n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        # now : hidden, cell : (num_layers, N, hidden_size) \n",
    "        \n",
    "        return encoder_states, hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42daaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM = 100\n",
    "# LATENT_DIM = 256\n",
    "# T decoder = 11\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size # LATENT_DIM\n",
    "        self.num_layers = num_layers  # 1 or 2\n",
    " \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # input size = vocab fr size = num_words_output\n",
    "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers)#), dropout=p) \n",
    "        # -> T decoder x N x hidden\n",
    "       \n",
    "        self.energy = nn.Linear(hidden_size*3,1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # ->  T x N x output size\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (N, 1), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0) # -> (1, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        # (sequence_length * num_layers, N, hidden_size)\n",
    "            \n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        # -> (sequence_length, N, 1)\n",
    "        attention = self.softmax(energy) \n",
    "        #(sequence_length, N, 1)\n",
    "        attention = attention.permute(1,2,0)\n",
    "        #(N, 1, sequence_length)\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        #(N, T, 2*hidden)\n",
    "            \n",
    "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        # (N, 1, 2*hidden) -> (1, N, 2*hidden)\n",
    "        \n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        # (1, N, 2*hidden + embedding_size) ->(1, N,  hidden)\n",
    "        # outputs shape: (1, N,  hidden)\n",
    "        # hidden, cell: (1, N,  hidden)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # -> (1, N, output_size)\n",
    "\n",
    "        # predictions shape: (N, 1, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "        # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c131bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, teacher_force_ratio):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_force_ratio = teacher_force_ratio\n",
    "    def forward(self, source, target):\n",
    "        # source = encoder_inputs\n",
    "        # target = encoder_inputs\n",
    "        batch_size = source.shape[1] # source (T_encoder, N)\n",
    "        target_len = target.shape[0] # target (T_decoder, N)\n",
    "        target_vocab_size = num_words_output # check this is correct num_words = len(word2idx_outputs) + 1 \n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device) # (T_decoder, N, vocab)\n",
    "\n",
    "        encoder_states, hidden, cell = self.encoder(source) # (num_layers, N, hidden_size) \n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0] # (1, N)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            #hidden, cell = hidden.squeeze(1), cell.squeeze(1)\n",
    "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < self.teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96fede43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "target_len: 33\n",
      "target_vocab_size: 13054\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        batch_size = encoder_in.shape[1] # source (T_encoder, N)\n",
    "        target_len = decoder_in.shape[0] # target (T_decoder, N)\n",
    "        target_vocab_size = num_words_output\n",
    "        print(\"batch_size:\", batch_size)\n",
    "        print(\"target_len:\", target_len)\n",
    "        print(\"target_vocab_size:\", target_vocab_size)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9701f9de-af6c-4a97-86c4-c47e5acfe4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53741ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_size_encoder = num_words\n",
    "input_size_decoder = num_words_output\n",
    "output_size = num_words_output\n",
    "encoder_embedding_size = EMBEDDING_DIM\n",
    "decoder_embedding_size = EMBEDDING_DIM\n",
    "hidden_size = LATENT_DIM  # Needs to be the same for both RNN's\n",
    "num_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "teacher_force_ratio = 1\n",
    "# Training hyperparameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eee8a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eeb0e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net, teacher_force_ratio).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d049655b-7944-4832-bdc6-51bdc5090fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./twitter_chatbot.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1ef8942-c395-4f6c-bd8f-36ae4b608437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (embedding): Embedding(9909, 200)\n",
       "  (rnn): LSTM(200, 512, bidirectional=True)\n",
       "  (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75ddda6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(9909, 200)\n",
       "    (rnn): LSTM(200, 512, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(13054, 200)\n",
       "    (rnn): LSTM(1224, 512)\n",
       "    (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "    (relu): ReLU()\n",
       "    (fc): Linear(in_features=512, out_features=13054, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-3)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3323154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 20]\n",
      "Loss: 0.7123\n",
      "[Epoch 1 / 20]\n",
      "Loss: 0.4790\n",
      "[Epoch 2 / 20]\n",
      "Loss: 0.8146\n",
      "[Epoch 3 / 20]\n",
      "Loss: 0.5808\n",
      "[Epoch 4 / 20]\n",
      "Loss: 0.5171\n",
      "[Epoch 5 / 20]\n",
      "Loss: 0.4896\n",
      "[Epoch 6 / 20]\n",
      "Loss: 0.3737\n",
      "[Epoch 7 / 20]\n",
      "Loss: 0.5228\n",
      "[Epoch 8 / 20]\n",
      "Loss: 0.6246\n",
      "[Epoch 9 / 20]\n",
      "Loss: 0.5195\n",
      "[Epoch 10 / 20]\n",
      "Loss: 0.7305\n",
      "[Epoch 11 / 20]\n",
      "Loss: 0.4083\n",
      "[Epoch 12 / 20]\n",
      "Loss: 0.6416\n",
      "[Epoch 13 / 20]\n",
      "Loss: 0.4529\n",
      "[Epoch 14 / 20]\n",
      "Loss: 0.3918\n",
      "[Epoch 15 / 20]\n",
      "Loss: 0.4786\n",
      "[Epoch 16 / 20]\n",
      "Loss: 0.6278\n",
      "[Epoch 17 / 20]\n",
      "Loss: 0.7785\n",
      "[Epoch 18 / 20]\n",
      "Loss: 0.3951\n",
      "[Epoch 19 / 20]\n",
      "Loss: 0.5411\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love dogs\"\n",
    "\n",
    "num_epochs = 20\n",
    "model.teacher_force_ratio = 0.8\n",
    "train_losses = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    #checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    #save_checkpoint(checkpoint)\n",
    "\n",
    "    #model.eval()\n",
    "\n",
    "    \n",
    "    #translated_sentence = translate_sentence(sentence)\n",
    "\n",
    "    #print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        #inp_data = batch.src.to(device)\n",
    "        #target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(encoder_in, decoder_in)\n",
    "\n",
    "        # Output is of shape (batch_size, trg_len, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have batch_size * output_words that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        #output = output[1:].reshape(-1, output.shape[2])\n",
    "        #target = target[1:].reshape(-1)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target.long())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    epoch_loss = np.mean(train_loss)           \n",
    "    train_losses[epoch] = epoch_loss\n",
    "    print(f'Loss: {epoch_loss:.4f}')\n",
    "        # Plot to tensorboard\n",
    "        #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        #step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2280fd65-c6bd-4fa0-adf1-96c4ffaa1efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some plots\n",
    "plt.plot(train_losses, label=\"train_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bc144df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './twitter_chatbot.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ace5940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map indexes back into real words\n",
    "# so wwe can view the results\n",
    "idx2word_eng = {v: w for w, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v: w for w, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f52db17d-e806-41cf-99a2-dca07f1ae166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # encode the input as state vectors.\n",
    "    input_seq = input_seq.to(device)\n",
    "    with torch.no_grad():\n",
    "        encoder_states, h, c = encoder_net(input_seq)\n",
    "\n",
    "        # generate empty target seq of length 1\n",
    "        target_seq = torch.zeros(1).int().to(device)\n",
    "\n",
    "        # populate the first character of target sequence with the start character\n",
    "        # NOTE: tokenizer lower cases all words\n",
    "        target_seq[0] = word2idx_outputs[\"<sos>\"]\n",
    "\n",
    "        # if we get this we break\n",
    "        eos = word2idx_outputs[\"<eos>\"]\n",
    "\n",
    "        # create translation\n",
    "        output_sentence = []\n",
    "        for _ in range(max_len_target):\n",
    "            output_tokens, h, c = decoder_net(target_seq, encoder_states, h, c)\n",
    "\n",
    "            # get next word\n",
    "            idx = output_tokens.argmax(1).item()\n",
    "\n",
    "            # end of sentence EOS\n",
    "            if eos == idx:\n",
    "                break\n",
    "\n",
    "            word = \"\"\n",
    "            if idx > 0:\n",
    "                word = idx2word_trans[idx]\n",
    "                output_sentence.append(word)\n",
    "\n",
    "            # update the decoder input\n",
    "            # which is just the word just generated\n",
    "            target_seq[0] = idx\n",
    "            #states_value = [h, c]\n",
    "\n",
    "        return \" \".join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d55bba23-35e4-4eb0-a2d4-57c388d17104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: lol\n",
      "Predicted: got fans\n",
      "True answer: sooo we getting that business proposal together? \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: were off to see the vampire,the wonderful vampire of oz!because, because, because, because,because of the wonderful blood she draws!\n",
      "Predicted: pleased with thinking for the world the to the where has written to the\n",
      "True answer: absurdly pleased with myself for thinking of this on the way to the lab. \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: problem is kadri is overpaid. no way anyone takes that contract.\n",
      "Predicted: he call out of out even i think out a more who who who a with who real a more a role\n",
      "True answer: what? he has a great contract! he is a 2- c unlike #little who is a 3+ with much more grit and more offence. \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: the trade off is that we have people who are actually educated voting. that is actually exactly what they had in mind.\n",
      "Predicted: is what you do trade when are away rights, away rights, something crucial voting. crucial voting. crucial voting.\n",
      "True answer: there is no trade off when you are taking away people's rights, especially something as crucial as voting. \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] n\n"
     ]
    }
   ],
   "source": [
    "model.teacher_force_ratio = 1\n",
    "encoder_net = model.encoder\n",
    "decoder_net = model.decoder\n",
    "while True:\n",
    "    # do some translation\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i : i + 1]\n",
    "    input_seq = torch.from_numpy(input_seq).to(device).permute(1,0)\n",
    "    pred_answer = decode_sequence(input_seq)\n",
    "    true_answer = target_texts[i]\n",
    "    print(\"_\")\n",
    "    print(\"Input:\", input_texts[i])\n",
    "    print(\"Predicted:\", pred_answer)\n",
    "    print(\"True answer:\", true_answer[:-5])\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith(\"n\"):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc7ee253-758a-4e54-a12c-4384eea0e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence=\"I love dogs\"):\n",
    "    sentence = [sentence]\n",
    "    sequence = tokenizer_inputs.texts_to_sequences(sentence)\n",
    "    sequence = torch.Tensor(sequence).int()\n",
    "    translation = decode_sequence(sequence)\n",
    "    print(translation)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20276f79-a701-4dfe-9d02-10cebc0d9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging stuff\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        encoder_states, hidden, cell = encoder_net(encoder_in)\n",
    "        \n",
    "        x = decoder_in[0]\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = decoder_net.dropout(decoder_net.embedding(x))\n",
    "        print(x.shape)\n",
    "        print(\"embedding shape:\", embedding.shape)\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        print(\"sequence_length:\", sequence_length)\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        print(\"h_reshaped.shape:\", h_reshaped.shape)\n",
    "        energy = decoder_net.relu(decoder_net.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        print(\"energy.shape:\", energy.shape)\n",
    "        attention = decoder_net.softmax(energy) \n",
    "        attention = attention.permute(1,2,0)\n",
    "        print(\"attention.shape:\", attention.shape)\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        print(\"encoder_states.shape:\", encoder_states.shape)\n",
    "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        print(\"context_vector shape:\",context_vector.shape)\n",
    "        print(\"embedding shape:\", embedding.shape)\n",
    "        #predictions, hidden, cell = decoder_net(x, encoder_states, hidden, cell)\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        print(\"rnn_input.shape:\",rnn_input.shape)\n",
    "        outputs, (hidden, cell) = decoder_net.rnn(rnn_input, (hidden, cell))\n",
    "        print(\"outputs.shape:\",outputs.shape )\n",
    "        print(\"hidden.shape:\",hidden.shape )\n",
    "        print(\"cell.shape:\",cell.shape )\n",
    "        predictions = decoder_net.fc(outputs).squeeze(0)\n",
    "        print(\"predictions.shape:\",predictions.shape )\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fe001d2-69ac-4805-8ec1-ddc84ec598e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and then that mouse had the nerve to try to eat our kibble!  let this be a lesson fur all the other mousies! <eos>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ca5cd-3eef-4515-932a-c9b882c22480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
