{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "uWUkTC5n3yRA"
      },
      "id": "uWUkTC5n3yRA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzzaxTNVgSQa"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import sys, re, os\n",
        "import string, unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "id": "XzzaxTNVgSQa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT_YT-G0gWpA",
        "outputId": "17219633-e1b8-4549-de79-7bff41ec20c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ],
      "id": "KT_YT-G0gWpA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google drive and Config"
      ],
      "metadata": {
        "id": "Fsgqjlhp33c9"
      },
      "id": "Fsgqjlhp33c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O8V4NPrgY3n",
        "outputId": "5c69b62f-8a80-48d9-cf90-bbc441fbe685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "2O8V4NPrgY3n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSMgGn7vgdfK"
      },
      "outputs": [],
      "source": [
        "drive_path = \"/content/drive/MyDrive/Sequence-Analysis-Projects\"\n",
        "twitter_path = os.path.join(drive_path, 'twitter-chatbot')\n",
        "if not os.path.exists(drive_path):\n",
        "    os.mkdir(drive_path)\n",
        "\n",
        "projects = ['translation', 'twitter-chatbot']\n",
        "for project in projects:\n",
        "    project_path = os.path.join(drive_path, project)\n",
        "    if not os.path.exists(project_path):\n",
        "        os.mkdir(project_path)"
      ],
      "id": "kSMgGn7vgdfK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9df92808"
      },
      "outputs": [],
      "source": [
        "# some configuration\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "MAX_NUM_WORDS = 20_000\n",
        "NUM_SAMPLES = 10_000\n",
        "EMBEDDING_DIM = 200\n",
        "VALIDATION_SPLIT = 0.2\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "LATENT_DIM = 512\n",
        "\n",
        "bos = \"<bos>\"\n",
        "eos = \"<eos>\"\n",
        "pad = \"<pad>\"\n",
        "unk = \"<unk>\""
      ],
      "id": "9df92808"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "nRQKfwWR34nb"
      },
      "id": "nRQKfwWR34nb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnB-1DAEgpZ-"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )"
      ],
      "id": "BnB-1DAEgpZ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4_-pfwmgsBo"
      },
      "outputs": [],
      "source": [
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "id": "j4_-pfwmgsBo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Text Preprocessing"
      ],
      "metadata": {
        "id": "nq8DeF9e38o9"
      },
      "id": "nq8DeF9e38o9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFg0M62Sgt0K",
        "outputId": "1e9a6ed4-66fb-48f4-b5d0-231f6dc1ea88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num samples: 8490\n"
          ]
        }
      ],
      "source": [
        "# load in the data\n",
        "input_texts = []  # sentence in original language\n",
        "target_texts = []  # sentence in target language\n",
        "#target_text_inputs = []  #  sentence in target language offset by 1\n",
        "t = 0\n",
        "for line in open(os.path.join(twitter_path, \"twitter_tab_format.txt\")):\n",
        "    # only keep a limited number of samples\n",
        "    t += 1\n",
        "    if t > NUM_SAMPLES:\n",
        "        break\n",
        "    line = line.rstrip()\n",
        "    # input and targets are separeted by tab\n",
        "    if \"\\t\" not in line:\n",
        "        continue\n",
        "\n",
        "    # split up the input and translation\n",
        "    input_text, target_text = line.split(\"\\t\")[:2]\n",
        "\n",
        "    input_text, target_text = normalizeString(input_text), normalizeString(target_text)\n",
        "  \n",
        "    #target_text_input = \"<sos> \" + translation\n",
        "\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    #target_text_inputs.append(target_text_input)\n",
        "print(\"num samples:\", len(input_texts))"
      ],
      "id": "RFg0M62Sgt0K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocab"
      ],
      "metadata": {
        "id": "BHr2jp8w4AmX"
      },
      "id": "BHr2jp8w4AmX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n83RaQ2KhIU6"
      },
      "outputs": [],
      "source": [
        "def build_vocab(data):\n",
        "    vocab = []\n",
        "    for sentence in data:\n",
        "        sentence_tokenized = word_tokenize(sentence)\n",
        "        for word in sentence_tokenized:\n",
        "            if word not in vocab:\n",
        "                vocab.append(word)\n",
        "\n",
        "    return vocab"
      ],
      "id": "n83RaQ2KhIU6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N0TsuuPhI_M",
        "outputId": "bfd43ffa-efcd-420b-9a11-290603992d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is 9571 token in english_vocab\n"
          ]
        }
      ],
      "source": [
        "english_vocab = build_vocab(input_texts)\n",
        "#english_vocab = build_vocab(train_inputs)\n",
        "print(f\"There is {len(english_vocab)} token in english_vocab\")"
      ],
      "id": "0N0TsuuPhI_M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1TsYYoShMnG"
      },
      "outputs": [],
      "source": [
        "english_vocab = [pad, unk, bos, eos] + english_vocab\n"
      ],
      "id": "Z1TsYYoShMnG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8riR5a7shP5r"
      },
      "outputs": [],
      "source": [
        "word2idx_eng = {}\n",
        "for i in range(len(english_vocab)):\n",
        "    word2idx_eng[english_vocab[i]] = i\n",
        "\n",
        "# map indexes back into real words\n",
        "# so wwe can view the results\n",
        "idx2word_eng = {v: w for w, v in word2idx_eng.items()}\n"
      ],
      "id": "8riR5a7shP5r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-FIaPfjheCo"
      },
      "outputs": [],
      "source": [
        "def build_sequences(texts, word2idx):\n",
        "    sequences = []\n",
        "    for sentence in texts:\n",
        "        sequence = []\n",
        "        sentence_tokenized = word_tokenize(sentence)\n",
        "        for token in sentence_tokenized:\n",
        "            if token in word2idx.keys():\n",
        "                sequence.append(word2idx[token])\n",
        "            else:\n",
        "                sequence.append(word2idx[unk])\n",
        "        sequences.append(sequence)\n",
        "    return sequences\n"
      ],
      "id": "f-FIaPfjheCo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGxy-0OJ9Wj6"
      },
      "source": [
        "# Build train/test set"
      ],
      "id": "pGxy-0OJ9Wj6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkF5vMnq9Z0D",
        "outputId": "eb3b7493-816b-41ad-9cf1-6d6b07ea4c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_inputs): 6792\n",
            "len(test_inputs): 1698\n"
          ]
        }
      ],
      "source": [
        "N = len(input_texts)\n",
        "indices = np.random.permutation(N)\n",
        "\n",
        "N_train = 4 * N // 5\n",
        "\n",
        "train_indices = list(indices[:N_train])\n",
        "test_indices = list(indices[N_train:])\n",
        "\n",
        "train_inputs = [input_texts[i] for i in train_indices]\n",
        "train_targets = [target_texts[i] for i in train_indices]\n",
        "\n",
        "test_inputs = [input_texts[i] for i in test_indices]\n",
        "test_targets = [target_texts[i] for i in test_indices]\n",
        "\n",
        "\n",
        "print(\"len(train_inputs):\", len(train_inputs))\n",
        "print(\"len(test_inputs):\", len(test_inputs))"
      ],
      "id": "tkF5vMnq9Z0D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0utsgNGBZmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5195b53-245d-469a-a737-7469dcb74ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(input_sequences_train): 6792\n",
            "len(input_sequences_test): 1698\n"
          ]
        }
      ],
      "source": [
        "#input_sequences = build_sequences(input_texts, word2idx_eng)\n",
        "#target_sequences = build_sequences(target_texts, word2idx_eng)\n",
        "\n",
        "input_sequences_train = build_sequences(train_inputs, word2idx_eng)\n",
        "target_sequences_train = build_sequences(train_targets, word2idx_eng)\n",
        "\n",
        "input_sequences_test = build_sequences(test_inputs, word2idx_eng)\n",
        "target_sequences_test = build_sequences(test_targets, word2idx_eng)\n",
        "\n",
        "print(\"len(input_sequences_train):\", len(input_sequences_train))\n",
        "print(\"len(input_sequences_test):\", len(input_sequences_test))\n",
        "\n",
        "#print(\"len(input_sequences):\", len(input_sequences))\n",
        "#print(\"len(target_sequences):\", len(target_sequences))"
      ],
      "id": "P0utsgNGBZmQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD_t-rtJhpVc"
      },
      "outputs": [],
      "source": [
        "bos_idx = word2idx_eng[bos]\n",
        "eos_idx = word2idx_eng[eos]\n",
        "#input_sequences = [[bos_idx] + sequence + [eos_idx] for sequence in input_sequences]\n",
        "# target_sequences_inputs = [[bos_idx] + sequence for sequence in target_sequences]\n",
        "# target_sequences = [sequence + [eos_idx] for sequence in target_sequences]\n",
        "\n",
        "target_sequences_train_inputs = [[bos_idx] + sequence for sequence in target_sequences_train]\n",
        "target_sequences_train = [sequence + [eos_idx] for sequence in target_sequences_train]\n",
        "\n",
        "\n",
        "target_sequences_test_inputs = [[bos_idx] + sequence for sequence in target_sequences_test]\n",
        "target_sequences_test = [sequence + [eos_idx] for sequence in target_sequences_test]"
      ],
      "id": "sD_t-rtJhpVc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoGoi3AYhw5c",
        "outputId": "45e95f25-9cd2-445d-8857-457c71f5b32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_len_input: 43\n"
          ]
        }
      ],
      "source": [
        "#max_len_input = max(len(s) for s in input_sequences)\n",
        "max_len_input = max(len(s) for s in input_sequences_train)\n",
        "\n",
        "print(\"max_len_input:\", max_len_input)"
      ],
      "id": "RoGoi3AYhw5c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeGj2265h1MC",
        "outputId": "cecf3a1f-9b3b-45ca-eb7e-93b7e20e5251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_len_target: 38\n"
          ]
        }
      ],
      "source": [
        "max_len_target = max(len(s) for s in target_sequences_train)\n",
        "#max_len_target = max(len(s) for s in target_sequences)\n",
        "\n",
        "print(\"max_len_target:\", max_len_target)"
      ],
      "id": "jeGj2265h1MC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Pre trained embedding"
      ],
      "metadata": {
        "id": "U5H5PvPw4JKZ"
      },
      "id": "U5H5PvPw4JKZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tHJo_q4iYQ1",
        "outputId": "e97eb906-8f83-42fd-fff7-be721495e25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-01 15:46:01--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-05-01 15:46:01--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-05-01 15:46:01--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 40s  \n",
            "\n",
            "2022-05-01 15:48:42 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -qq glove.6B.zip\n"
      ],
      "id": "1tHJo_q4iYQ1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f883a42",
        "outputId": "3f8063d1-100f-4559-c92c-39292734193a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading word vectors ...\n",
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# load in pre-trained word vectors\n",
        "print(\"loading word vectors ...\")\n",
        "word2vec_path = 'glove.6B.%sd.txt'\n",
        "word2vec = {}\n",
        "with open(\n",
        "    os.path.join(word2vec_path % EMBEDDING_DIM)\n",
        ") as f:\n",
        "    # is just a space-separated text file in the format:\n",
        "    # word vec[0] vec[1] vec[2]\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vec = np.array(values[1:], dtype=\"float32\")\n",
        "        word2vec[word] = vec\n",
        "    print(\"Found %s word vectors.\" % len(word2vec))"
      ],
      "id": "9f883a42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "909b1ede",
        "outputId": "deae2ff9-e8a4-44d4-991e-5d5a46dbf869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filling pre-trained embeddings...\n"
          ]
        }
      ],
      "source": [
        "# prepare embedding matrix\n",
        "print(\"Filling pre-trained embeddings...\")\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_eng))\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx_eng.items():\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "id": "909b1ede"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Interface"
      ],
      "metadata": {
        "id": "KxJEIFut4MwO"
      },
      "id": "KxJEIFut4MwO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztbQeQKNinPI"
      },
      "outputs": [],
      "source": [
        "# build data set\n",
        "class TwitterDatset(data.Dataset):\n",
        "    def __init__(self, max_len_input, max_len_target,input_sequences, target_sequences, target_sequences_inputs):\n",
        "        self.max_len_input = max_len_input\n",
        "        self.max_len_target = max_len_target\n",
        "        self.input_sequences = input_sequences\n",
        "        self.target_sequences = target_sequences\n",
        "        self.target_sequences_inputs = target_sequences_inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_sequences)\n",
        "\n",
        "    def pad_input(self, sequence):\n",
        "        max_len = self.max_len_input\n",
        "        sequence_len = len(sequence)\n",
        "        if sequence_len >= max_len:\n",
        "            sequence = sequence[:max_len]\n",
        "        else:\n",
        "            sequence = [0] * (max_len - sequence_len) + sequence\n",
        "        \n",
        "        return sequence\n",
        "\n",
        "    def pad_target(self, sequence):\n",
        "        max_len = self.max_len_target\n",
        "        sequence_len = len(sequence)\n",
        "        if sequence_len >= max_len:\n",
        "            sequence = sequence[:max_len]\n",
        "        else:\n",
        "            sequence = sequence + [0] * (max_len - sequence_len)  \n",
        "        return sequence\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        encoder_in = self.pad_input(self.input_sequences[idx])\n",
        "        decoder_in = self.pad_target(self.target_sequences_inputs[idx])\n",
        "        decoder_out = self.pad_target(self.target_sequences[idx])\n",
        "\n",
        "        encoder_in = torch.LongTensor(encoder_in)\n",
        "        decoder_in = torch.LongTensor(decoder_in)\n",
        "        decoder_out = torch.FloatTensor(decoder_out)\n",
        "\n",
        "    \n",
        "        return encoder_in, decoder_in, decoder_out\n"
      ],
      "id": "ztbQeQKNinPI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Setup "
      ],
      "metadata": {
        "id": "bRE-BIwG4X_r"
      },
      "id": "bRE-BIwG4X_r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b00baa7c"
      },
      "outputs": [],
      "source": [
        "#twitter_dataset = TwitterDatset(max_len_input, max_len_target, input_sequences, target_sequences, target_sequences_inputs)\n",
        "\n",
        "train_dataset = TwitterDatset(max_len_input, max_len_target, input_sequences_train, target_sequences_train, target_sequences_train_inputs)\n",
        "test_dataset = TwitterDatset(max_len_input, max_len_target, input_sequences_test, target_sequences_test, target_sequences_test_inputs)"
      ],
      "id": "b00baa7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911971a7"
      },
      "outputs": [],
      "source": [
        "# data_loader = torch.utils.data.DataLoader(\n",
        "#     dataset=twitter_dataset,\n",
        "#     shuffle=True,\n",
        "#     batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "id": "911971a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "386f5aad"
      },
      "outputs": [],
      "source": [
        "# load pre-trained word embeddings into an embedding layer\n",
        "# freeze the layer\n",
        "embedding_layer = nn.Embedding(num_words, EMBEDDING_DIM)  # vocab size  # embedding dim\n",
        "embedding_layer.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
        "embedding_layer.requires_grad = False\n"
      ],
      "id": "386f5aad"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "4DslDCen4atP"
      },
      "id": "4DslDCen4atP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de69bbe7"
      },
      "outputs": [],
      "source": [
        "# some configuration\n",
        "# EMBEDDING_DIM = 100\n",
        "# LATENT_DIM = 256\n",
        "# T encoder = 4\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size,embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size # LATENT_DIM\n",
        "        self.num_layers = num_layers # 1 or 2\n",
        "\n",
        "        self.embedding = embedding_layer # vocab size x EMBEDDING_DIM\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)#, dropout=p) # -> T x N x 2*hidden\n",
        "\n",
        "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (N, T encoder) where N is batch size\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x)) # (T encoder, N, EMBEDDING_DIM)\n",
        "        # embedding shape: # (T encoder, N, EMBEDDING_DIM)\n",
        "\n",
        "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
        "        #  encoder_states shape:  (T encoder *num_layers , N, 2*hidden_size) \n",
        "        # hidden, cell : (2*num_layers, N, hidden_size) bidirectional = True\n",
        "        \n",
        "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
        "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
        "        # now : hidden, cell : (num_layers, N, hidden_size) \n",
        "        \n",
        "        return encoder_states, hidden, cell\n",
        "    "
      ],
      "id": "de69bbe7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42daaf60"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size # LATENT_DIM\n",
        "        self.num_layers = num_layers  # 1 or 2\n",
        " \n",
        "        self.embedding = nn.Embedding(input_size, embedding_size) # input size = vocab fr size = num_words_output\n",
        "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers)#), dropout=p) \n",
        "        # -> T decoder x N x hidden\n",
        "       \n",
        "        self.energy = nn.Linear(hidden_size*3,1)\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(hidden_size, output_size) # ->  T x N x output size\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, x, encoder_states, hidden, cell):\n",
        "        # x shape: (N) where N is for batch size, we want it to be (N, 1), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x = x.unsqueeze(0) # -> (1, N)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        sequence_length = encoder_states.shape[0]\n",
        "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
        "        # (sequence_length * num_layers, N, hidden_size)\n",
        "            \n",
        "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
        "        # -> (sequence_length, N, 1)\n",
        "        attention = self.softmax(energy) \n",
        "        #(sequence_length, N, 1)\n",
        "        attention = attention.permute(1,2,0)\n",
        "        #(N, 1, sequence_length)\n",
        "        encoder_states = encoder_states.permute(1,0,2)\n",
        "        #(N, T, 2*hidden)\n",
        "            \n",
        "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
        "        # (N, 1, 2*hidden) -> (1, N, 2*hidden)\n",
        "        \n",
        "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
        "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
        "        # (1, N, 2*hidden + embedding_size) ->(1, N,  hidden)\n",
        "        # outputs shape: (1, N,  hidden)\n",
        "        # hidden, cell: (1, N,  hidden)\n",
        "\n",
        "        predictions = self.fc(outputs)\n",
        "        # -> (1, N, output_size)\n",
        "\n",
        "        # predictions shape: (N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell\n",
        "        # \n"
      ],
      "id": "42daaf60"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1c131bc"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, teacher_force_ratio=1):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.teacher_force_ratio = teacher_force_ratio\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        # source = encoder_inputs\n",
        "        # target = encoder_inputs\n",
        "        batch_size = source.shape[1] # source (T_encoder, N)\n",
        "        target_len = target.shape[0] # target (T_decoder, N)\n",
        "        target_vocab_size = len(word2idx_eng) # check this is correct num_words = len(word2idx_outputs) + 1 \n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device) # (T_decoder, N, vocab)\n",
        "\n",
        "        encoder_states, hidden, cell = self.encoder(source) # (num_layers, N, hidden_size) \n",
        "\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0] # (1, N)\n",
        "        outputs[-1] = word2idx_eng[eos] * torch.ones(batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            #hidden, cell = hidden.squeeze(1), cell.squeeze(1)\n",
        "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t-1] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            # With probability of teacher_force_ratio we take the actual next word\n",
        "            # otherwise we take the word that the Decoder predicted it to be.\n",
        "            # Teacher Forcing is used so that the model gets used to seeing\n",
        "            # similar inputs at training and testing time, if teacher forcing is 1\n",
        "            # then inputs at test time might be completely different than what the\n",
        "            # network is used to. This was a long comment.\n",
        "            x = target[t] if random.random() < self.teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs\n"
      ],
      "id": "a1c131bc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training setup"
      ],
      "metadata": {
        "id": "15qtNvRt4f4a"
      },
      "id": "15qtNvRt4f4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOPPAA3ONpiC",
        "outputId": "28d6e05f-b64e-418e-8852-c082892e503c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ],
      "id": "SOPPAA3ONpiC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53741ff1"
      },
      "outputs": [],
      "source": [
        "input_size_encoder = num_words\n",
        "input_size_decoder = num_words\n",
        "output_size = num_words\n",
        "encoder_embedding_size = EMBEDDING_DIM\n",
        "decoder_embedding_size = EMBEDDING_DIM\n",
        "hidden_size = LATENT_DIM  # Needs to be the same for both RNN's\n",
        "num_layers = 1\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5\n",
        "teacher_force_ratio = 0.5\n",
        "# Training hyperparameters\n",
        "num_epochs = 300\n",
        "learning_rate = 0.001\n",
        "batch_size = 64"
      ],
      "id": "53741ff1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eee8a8b2"
      },
      "outputs": [],
      "source": [
        "encoder_net = Encoder(\n",
        "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
        ").to(device)\n",
        "\n",
        "decoder_net = Decoder(\n",
        "    input_size_decoder,\n",
        "    decoder_embedding_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dec_dropout,\n",
        ").to(device)"
      ],
      "id": "eee8a8b2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeb0e2d8"
      },
      "outputs": [],
      "source": [
        "model = Seq2Seq(encoder_net, decoder_net, teacher_force_ratio).to(device)"
      ],
      "id": "eeb0e2d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d049655b-7944-4832-bdc6-51bdc5090fe1"
      },
      "outputs": [],
      "source": [
        "model = torch.load('./twitter_chatbot.pth')"
      ],
      "id": "d049655b-7944-4832-bdc6-51bdc5090fe1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ddda6f",
        "outputId": "e892d26c-b2aa-49df-bd99-c01ff937797f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(9575, 200)\n",
              "    (rnn): LSTM(200, 512, bidirectional=True)\n",
              "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(9575, 200)\n",
              "    (rnn): LSTM(1224, 512)\n",
              "    (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
              "    (softmax): Softmax(dim=0)\n",
              "    (relu): ReLU()\n",
              "    (fc): Linear(in_features=512, out_features=9575, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# loss and optimizer \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)#, weight_decay=1e-3)\n",
        "model.train()"
      ],
      "id": "75ddda6f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train loop"
      ],
      "metadata": {
        "id": "n_DSCRfZ4jiD"
      },
      "id": "n_DSCRfZ4jiD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3323154a",
        "outputId": "e35788b9-617c-4b1c-bba3-1fff006a0f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1 / 100]\n",
            "Train Loss: 2.889, Test Loss: 2.251\n",
            "[Epoch 2 / 100]\n",
            "Train Loss: 2.864, Test Loss: 2.534\n",
            "[Epoch 3 / 100]\n",
            "Train Loss: 2.880, Test Loss: 2.204\n",
            "[Epoch 4 / 100]\n",
            "Train Loss: 2.360, Test Loss: 2.185\n",
            "[Epoch 5 / 100]\n",
            "Train Loss: 1.958, Test Loss: 2.112\n",
            "[Epoch 6 / 100]\n",
            "Train Loss: 1.738, Test Loss: 2.099\n",
            "[Epoch 7 / 100]\n",
            "Train Loss: 1.592, Test Loss: 2.296\n",
            "[Epoch 8 / 100]\n",
            "Train Loss: 2.394, Test Loss: 2.099\n",
            "[Epoch 9 / 100]\n",
            "Train Loss: 3.012, Test Loss: 2.214\n",
            "[Epoch 10 / 100]\n",
            "Train Loss: 2.220, Test Loss: 2.165\n",
            "[Epoch 11 / 100]\n",
            "Train Loss: 2.260, Test Loss: 2.227\n",
            "[Epoch 12 / 100]\n",
            "Train Loss: 1.948, Test Loss: 2.191\n",
            "[Epoch 13 / 100]\n",
            "Train Loss: 1.730, Test Loss: 2.246\n",
            "[Epoch 14 / 100]\n",
            "Train Loss: 2.288, Test Loss: 2.237\n",
            "[Epoch 15 / 100]\n",
            "Train Loss: 2.454, Test Loss: 2.250\n",
            "[Epoch 16 / 100]\n",
            "Train Loss: 1.978, Test Loss: 2.178\n",
            "[Epoch 17 / 100]\n",
            "Train Loss: 2.637, Test Loss: 2.265\n",
            "[Epoch 18 / 100]\n",
            "Train Loss: 1.898, Test Loss: 2.166\n",
            "[Epoch 19 / 100]\n",
            "Train Loss: 1.148, Test Loss: 2.288\n",
            "[Epoch 20 / 100]\n",
            "Train Loss: 1.794, Test Loss: 2.284\n",
            "[Epoch 21 / 100]\n",
            "Train Loss: 2.614, Test Loss: 2.237\n",
            "[Epoch 22 / 100]\n",
            "Train Loss: 1.941, Test Loss: 2.278\n",
            "[Epoch 23 / 100]\n",
            "Train Loss: 2.399, Test Loss: 2.264\n",
            "[Epoch 24 / 100]\n",
            "Train Loss: 1.121, Test Loss: 2.277\n",
            "[Epoch 25 / 100]\n",
            "Train Loss: 2.106, Test Loss: 2.377\n",
            "[Epoch 26 / 100]\n",
            "Train Loss: 1.591, Test Loss: 2.277\n",
            "[Epoch 27 / 100]\n",
            "Train Loss: 1.559, Test Loss: 2.408\n",
            "[Epoch 28 / 100]\n",
            "Train Loss: 1.698, Test Loss: 2.464\n",
            "[Epoch 29 / 100]\n",
            "Train Loss: 1.699, Test Loss: 2.363\n",
            "[Epoch 30 / 100]\n",
            "Train Loss: 1.839, Test Loss: 2.231\n",
            "[Epoch 31 / 100]\n",
            "Train Loss: 1.511, Test Loss: 2.370\n",
            "[Epoch 32 / 100]\n",
            "Train Loss: 1.954, Test Loss: 2.246\n",
            "[Epoch 33 / 100]\n",
            "Train Loss: 1.195, Test Loss: 2.337\n",
            "[Epoch 34 / 100]\n",
            "Train Loss: 1.335, Test Loss: 2.282\n",
            "[Epoch 35 / 100]\n",
            "Train Loss: 1.608, Test Loss: 2.373\n",
            "[Epoch 36 / 100]\n",
            "Train Loss: 1.850, Test Loss: 2.410\n",
            "[Epoch 37 / 100]\n",
            "Train Loss: 1.387, Test Loss: 2.400\n",
            "[Epoch 38 / 100]\n",
            "Train Loss: 1.701, Test Loss: 2.527\n",
            "[Epoch 39 / 100]\n",
            "Train Loss: 1.239, Test Loss: 2.668\n",
            "[Epoch 40 / 100]\n",
            "Train Loss: 0.933, Test Loss: 2.381\n",
            "[Epoch 41 / 100]\n",
            "Train Loss: 1.510, Test Loss: 2.443\n",
            "[Epoch 42 / 100]\n",
            "Train Loss: 1.333, Test Loss: 2.570\n",
            "[Epoch 43 / 100]\n",
            "Train Loss: 1.466, Test Loss: 2.418\n",
            "[Epoch 44 / 100]\n",
            "Train Loss: 1.723, Test Loss: 2.648\n",
            "[Epoch 45 / 100]\n",
            "Train Loss: 1.085, Test Loss: 2.680\n",
            "[Epoch 46 / 100]\n",
            "Train Loss: 0.857, Test Loss: 2.822\n",
            "[Epoch 47 / 100]\n",
            "Train Loss: 1.051, Test Loss: 2.498\n",
            "[Epoch 48 / 100]\n",
            "Train Loss: 1.196, Test Loss: 2.655\n",
            "[Epoch 49 / 100]\n",
            "Train Loss: 0.670, Test Loss: 2.795\n",
            "[Epoch 50 / 100]\n",
            "Train Loss: 0.944, Test Loss: 2.604\n",
            "[Epoch 51 / 100]\n",
            "Train Loss: 1.266, Test Loss: 2.593\n",
            "[Epoch 52 / 100]\n",
            "Train Loss: 1.219, Test Loss: 2.770\n",
            "[Epoch 53 / 100]\n",
            "Train Loss: 0.598, Test Loss: 2.755\n",
            "[Epoch 54 / 100]\n",
            "Train Loss: 1.078, Test Loss: 2.828\n",
            "[Epoch 55 / 100]\n",
            "Train Loss: 1.258, Test Loss: 2.965\n",
            "[Epoch 56 / 100]\n",
            "Train Loss: 0.704, Test Loss: 2.624\n",
            "[Epoch 57 / 100]\n",
            "Train Loss: 0.734, Test Loss: 2.863\n",
            "[Epoch 58 / 100]\n",
            "Train Loss: 1.258, Test Loss: 2.844\n",
            "[Epoch 59 / 100]\n",
            "Train Loss: 0.935, Test Loss: 3.067\n",
            "[Epoch 60 / 100]\n",
            "Train Loss: 1.036, Test Loss: 2.818\n",
            "[Epoch 61 / 100]\n",
            "Train Loss: 0.549, Test Loss: 2.755\n",
            "[Epoch 62 / 100]\n",
            "Train Loss: 1.007, Test Loss: 2.883\n",
            "[Epoch 63 / 100]\n",
            "Train Loss: 0.796, Test Loss: 2.822\n",
            "[Epoch 64 / 100]\n",
            "Train Loss: 0.485, Test Loss: 2.849\n",
            "[Epoch 65 / 100]\n",
            "Train Loss: 0.414, Test Loss: 3.163\n",
            "[Epoch 66 / 100]\n",
            "Train Loss: 0.829, Test Loss: 2.985\n",
            "[Epoch 67 / 100]\n",
            "Train Loss: 1.225, Test Loss: 3.073\n",
            "[Epoch 68 / 100]\n",
            "Train Loss: 0.770, Test Loss: 3.191\n",
            "[Epoch 69 / 100]\n",
            "Train Loss: 0.499, Test Loss: 2.844\n",
            "[Epoch 70 / 100]\n",
            "Train Loss: 0.653, Test Loss: 2.921\n",
            "[Epoch 71 / 100]\n",
            "Train Loss: 0.468, Test Loss: 2.930\n",
            "[Epoch 72 / 100]\n",
            "Train Loss: 0.465, Test Loss: 3.027\n",
            "[Epoch 73 / 100]\n",
            "Train Loss: 0.876, Test Loss: 3.092\n",
            "[Epoch 74 / 100]\n",
            "Train Loss: 0.567, Test Loss: 3.060\n",
            "[Epoch 75 / 100]\n",
            "Train Loss: 0.429, Test Loss: 3.184\n",
            "[Epoch 76 / 100]\n",
            "Train Loss: 0.560, Test Loss: 3.589\n",
            "[Epoch 77 / 100]\n",
            "Train Loss: 0.430, Test Loss: 3.270\n",
            "[Epoch 78 / 100]\n",
            "Train Loss: 0.617, Test Loss: 3.106\n",
            "[Epoch 79 / 100]\n",
            "Train Loss: 0.400, Test Loss: 3.217\n",
            "[Epoch 80 / 100]\n",
            "Train Loss: 0.919, Test Loss: 3.251\n",
            "[Epoch 81 / 100]\n",
            "Train Loss: 0.346, Test Loss: 3.160\n",
            "[Epoch 82 / 100]\n",
            "Train Loss: 0.896, Test Loss: 3.009\n",
            "[Epoch 83 / 100]\n",
            "Train Loss: 0.380, Test Loss: 3.142\n",
            "[Epoch 84 / 100]\n",
            "Train Loss: 0.428, Test Loss: 3.457\n",
            "[Epoch 85 / 100]\n",
            "Train Loss: 0.571, Test Loss: 3.314\n",
            "[Epoch 86 / 100]\n",
            "Train Loss: 0.524, Test Loss: 3.347\n",
            "[Epoch 87 / 100]\n",
            "Train Loss: 0.500, Test Loss: 3.379\n",
            "[Epoch 88 / 100]\n",
            "Train Loss: 0.458, Test Loss: 3.477\n",
            "[Epoch 89 / 100]\n",
            "Train Loss: 0.370, Test Loss: 3.312\n",
            "[Epoch 90 / 100]\n",
            "Train Loss: 0.408, Test Loss: 3.289\n",
            "[Epoch 91 / 100]\n",
            "Train Loss: 0.344, Test Loss: 3.340\n",
            "[Epoch 92 / 100]\n",
            "Train Loss: 0.311, Test Loss: 3.249\n",
            "[Epoch 93 / 100]\n",
            "Train Loss: 0.443, Test Loss: 3.209\n",
            "[Epoch 94 / 100]\n",
            "Train Loss: 0.317, Test Loss: 3.544\n",
            "[Epoch 95 / 100]\n",
            "Train Loss: 0.469, Test Loss: 3.391\n",
            "[Epoch 96 / 100]\n",
            "Train Loss: 0.358, Test Loss: 3.514\n",
            "[Epoch 97 / 100]\n",
            "Train Loss: 0.329, Test Loss: 3.730\n",
            "[Epoch 98 / 100]\n",
            "Train Loss: 0.398, Test Loss: 3.384\n",
            "[Epoch 99 / 100]\n",
            "Train Loss: 0.383, Test Loss: 3.401\n",
            "[Epoch 100 / 100]\n",
            "Train Loss: 0.266, Test Loss: 3.634\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "train_losses = np.zeros(num_epochs)\n",
        "test_losses = np.zeros(num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch+1} / {num_epochs}]\")\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        train_loss = []\n",
        "        # Get input and targets and get to cuda\n",
        "        # encoder_in, decoder_in, decoder_out\n",
        "        encoder_in, decoder_in, target = batch\n",
        "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
        "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
        "\n",
        "        # Forward prop\n",
        "        output = model(encoder_in, decoder_in)\n",
        "\n",
        "        # Output is of shape (batch_size, trg_len, output_dim) but Cross Entropy Loss\n",
        "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
        "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
        "        # way that we have batch_size * output_words that we want to send in into\n",
        "        # our cost function, so we need to do some reshapin. While we're at it\n",
        "        # Let's also remove the start token while we're at it\n",
        "        #output = output[1:].reshape(-1, output.shape[2])\n",
        "        #target = target[1:].reshape(-1)\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target.reshape(-1)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target.long())\n",
        "\n",
        "        # Back prop\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
        "        # within a healthy range\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        # Gradient descent step\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.item())\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_idx, batch in enumerate(test_loader):\n",
        "        test_loss = []\n",
        "        # Get input and targets and get to cuda\n",
        "        # encoder_in, decoder_in, decoder_out\n",
        "        encoder_in, decoder_in, target = batch\n",
        "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
        "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
        "  \n",
        "        # Forward prop\n",
        "        output = model(encoder_in, decoder_in)\n",
        "\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(output, target.long())\n",
        "        test_loss.append(loss.item())\n",
        "    \n",
        "    epoch_train_loss = np.mean(train_loss)           \n",
        "    train_losses[epoch] = epoch_train_loss\n",
        "\n",
        "    epoch_test_loss = np.mean(test_loss)           \n",
        "    test_losses[epoch] = epoch_test_loss\n",
        "\n",
        "    print(f'Train Loss: {epoch_train_loss:.3f}, Test Loss: {epoch_test_loss:.3f}')\n",
        "\n",
        "\n"
      ],
      "id": "3323154a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and Evaluation"
      ],
      "metadata": {
        "id": "rYg_Hgz34n5K"
      },
      "id": "rYg_Hgz34n5K"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "2280fd65-c6bd-4fa0-adf1-96c4ffaa1efe",
        "outputId": "d2131653-39a7-4bbc-d587-0de874d83d19"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# some plots\n",
        "plt.plot(train_losses, label=\"train_loss\")\n",
        "plt.plot(test_losses, label=\"test_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "2280fd65-c6bd-4fa0-adf1-96c4ffaa1efe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bc144df"
      },
      "outputs": [],
      "source": [
        "#torch.save(model, './twitter_chatbot.pth')\n",
        "torch.save(model, os.path.join(twitter_path, f\"twitter_chatbot.pth\"))\n"
      ],
      "id": "0bc144df"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating"
      ],
      "metadata": {
        "id": "zrdl9_pf43Ao"
      },
      "id": "zrdl9_pf43Ao"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f52db17d-e806-41cf-99a2-dca07f1ae166"
      },
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "def decode_sequence(input_seq, mode=\"greedy\"):\n",
        "    # encode the input as state vectors.\n",
        "    input_seq = input_seq.to(device)\n",
        "    with torch.no_grad():\n",
        "        encoder_states, h, c = encoder_net(input_seq)\n",
        "\n",
        "        # generate empty target seq of length 1\n",
        "        target_seq = torch.zeros(1).int().to(device)\n",
        "\n",
        "        # populate the first character of target sequence with the start character\n",
        "        # NOTE: tokenizer lower cases all words\n",
        "        target_seq[0] = word2idx_eng[bos]\n",
        "\n",
        "        # if we get this we break\n",
        "        eos_idx = word2idx_eng[eos]\n",
        "\n",
        "        # create translation\n",
        "        output_sentence = []\n",
        "        for _ in range(max_len_target):\n",
        "            output_tokens, h, c = decoder_net(target_seq, encoder_states, h, c)\n",
        "\n",
        "            if mode == \"sample\":\n",
        "                probs = softmax(output_tokens).view(-1)\n",
        "                idx = np.random.choice(len(probs), p=probs.detach().cpu().numpy())\n",
        "\n",
        "            else:\n",
        "                # get next word\n",
        "                idx = output_tokens.argmax(1).item()\n",
        "\n",
        "            # end of sentence EOS\n",
        "            if eos_idx == idx:\n",
        "                break\n",
        "\n",
        "            word = \"\"\n",
        "            if idx > 0:\n",
        "                word = idx2word_eng[idx]\n",
        "                output_sentence.append(word)\n",
        "\n",
        "            # update the decoder input\n",
        "            # which is just the word just generated\n",
        "            target_seq[0] = idx\n",
        "            #states_value = [h, c]\n",
        "\n",
        "        return \" \".join(output_sentence)"
      ],
      "id": "f52db17d-e806-41cf-99a2-dca07f1ae166"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers On train set"
      ],
      "metadata": {
        "id": "hWVlXkvT47ag"
      },
      "id": "hWVlXkvT47ag"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d55bba23-35e4-4eb0-a2d4-57c388d17104",
        "outputId": "2da40bd3-6d16-4e1d-efad-5e24989897d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_\n",
            "Input: whoa sorry there champ my kid goes there so i was looking for updates didnt know it was a problem reply u relax\n",
            "True answer: oh i apologize not sure what s happening there right now\n",
            "Predicted: oh i apologize not sure what s happening there right now\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: well since you re up . . . .\n",
            "True answer: lol noo who do u think i am your maid ?\n",
            "Predicted: lol noo who do u think i am your maid ?\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: starting today any brock excuses needs to stay off my tl . he needs more time not big enough sample size or any other bullshit go elsewhere\n",
            "True answer: he plays its not for football reasons . play the other guy . find out . stay with brock lose the team . they know .\n",
            "Predicted: he plays its not for football reasons . the the guy . . find out . stay out . brock lose the team . they know they\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: i blame trump for a lot of this tbh .\n",
            "True answer: the few who ve come into my mentions saying random ignorant shit have been trump supporters lmao\n",
            "Predicted: the few who ve come into my mentions saying random ignorant shit have been trump supporters lmao\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: because louisville would have destroyed penn state ! !\n",
            "True answer: i wouldn t be so sure about that . louisville wouldn t stand a chance against osu mich neb wisconsin . . . . .\n",
            "Predicted: i wouldn t be so sure about that . louisville wouldn t stand a chance against osu mich neb wisconsin . . . . . .\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: no it s not but then again i m still on season \n",
            "True answer: on top of it all that show is just weird lmao\n",
            "Predicted: on top of it all that show is just weird lmao\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: h \n",
            "True answer: i m pooped maybe later or tomorrow\n",
            "Predicted: i m <unk> maybe later or tomorrow\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: less then two hours of work and battery on phone\n",
            "True answer: oh dear\n",
            "Predicted: oh dear\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: perfectly lonely\n",
            "True answer: need some company ?\n",
            "Predicted: need some company ?\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: how many award winning dishes or tasting menus have cooked green pepper ? probably zero .\n",
            "True answer: like . . .bell pepper ? or any ?\n",
            "Predicted: like . . .bell pepper ? or any ?\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: if she ain t foreign then she boring\n",
            "True answer: ur a fag\n",
            "Predicted: ur a fag\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: still wide awake amp got to be up in three hours watching cher on youtube\n",
            "True answer: i m wide awake cause i m at work keeping the world safe amp saving lives lol\n",
            "Predicted: i m wide awake cause i m at work keeping the world safe amp saving lives lol\n",
            "Continue? [Y/n]n\n"
          ]
        }
      ],
      "source": [
        "encoder_net = model.encoder\n",
        "decoder_net = model.decoder\n",
        "while True:\n",
        "    # do some answering\n",
        "    i = np.random.choice(len(train_dataset))\n",
        "    input_seq, _, _ = train_dataset[i]\n",
        "    input_seq = input_seq.unsqueeze(1)\n",
        "    pred_answer = decode_sequence(input_seq, mode=\"sample\")\n",
        "    true_answer = train_targets[i]\n",
        "    print(\"_\")\n",
        "    print(\"Input:\", train_inputs[i])\n",
        "    print(\"True answer:\", true_answer)#[:-5])\n",
        "    print(\"Predicted:\", pred_answer)\n",
        "    \n",
        "\n",
        "    ans = input(\"Continue? [Y/n]\")\n",
        "    if ans and ans.lower().startswith(\"n\"):\n",
        "        break\n"
      ],
      "id": "d55bba23-35e4-4eb0-a2d4-57c388d17104"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers on test set"
      ],
      "metadata": {
        "id": "sV5pqgmh4-jB"
      },
      "id": "sV5pqgmh4-jB"
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    # do some answering\n",
        "    i = np.random.choice(len(test_dataset))\n",
        "    input_seq, _, _ = test_dataset[i]\n",
        "    input_seq = input_seq.unsqueeze(1)\n",
        "    pred_answer = decode_sequence(input_seq, mode=\"sample\")\n",
        "    true_answer = test_targets[i]\n",
        "    print(\"_\")\n",
        "    print(\"Input:\", test_inputs[i])\n",
        "    print(\"True answer:\", true_answer)#[:-5])\n",
        "    print(\"Predicted:\", pred_answer)\n",
        "    \n",
        "\n",
        "    ans = input(\"Continue? [Y/n]\")\n",
        "    if ans and ans.lower().startswith(\"n\"):\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_SOoB0idm8i",
        "outputId": "e2fd25ce-34d3-4b74-aee4-a5c5e9d3f0e5"
      },
      "id": "1_SOoB0idm8i",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_\n",
            "Input: i still need other stuff to do though\n",
            "True answer: idk quilting is time consuming af lol but a cooking class or art class or something ? i would die . it sounds so fun\n",
            "Predicted: they never get that cab\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: and both girls with him looked like barbie dolls from wwe too idk so many unanswered questioms\n",
            "True answer: one is his wife who was a former wwe wrestler .other one i think is ultimate warriors widow\n",
            "Predicted: people thing i all lot she s me cause . do with kassouf to play with the but at a big man\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: yes because seventeen comes after sixteen and you were born almost seventeen years ago\n",
            "True answer: lmao i still feel like a child\n",
            "Predicted: the this is why everyone while i m still there\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: no he replied he even sent another one\n",
            "True answer: ohhhh well then\n",
            "Predicted: if . was was just !\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: go\n",
            "True answer: it was just pudding\n",
            "Predicted: just look tomorrow thursday\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: i m hungry too\n",
            "True answer: im just gonna starve until morning\n",
            "Predicted: me ok ok let s get a drone then lmao my s for them\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: one of my other tweeps knows his identity . he s got an autograph signing business .\n",
            "True answer: good ! so if he follows through on his threats we know who he is . they seldom follow through but be careful .\n",
            "Predicted: wait i s not and white <unk> one who do that already seen the the young of the story of that way .\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: the one you see wearing the most . that s the one you should take care of best .\n",
            "True answer: true . i was bummed i couldn t get the camo tee .\n",
            "Predicted: plus all of least i thought it s the magic have no show .\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: answer me this fellow apple peoples how many times in the past year have you used the escape key ?\n",
            "True answer: about times today . terminal vim user .\n",
            "Predicted: you at least i can talk to like joe and figure a and they want to whatever a question and have\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: the new originals season came on to netflix and it reminded me of how much i love her\n",
            "True answer: and once you catch up the new season starts in january or february on cw ! ! ! !\n",
            "Predicted: and once you catch up the new season starts in january or february on cw ! ! ! !\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input:  reisaru fics doneeeeee\n",
            "True answer: yeeeeeey o \n",
            "Predicted: so much too easy ! lol\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: me too ! i hope it works out . i know that it s going to be hard for her to serve because she won t be respected by some .\n",
            "True answer: hillary works well with everyone . she s going to surprise a lot of people .\n",
            "Predicted: he only nobody t buy anything call .\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: what kinda bias ? lol\n",
            "True answer: i just think he s a beautiful man haha . trainspotting good will hunting snow on tha bluff the imitation game american beauty\n",
            "Predicted: the some <unk> was still nothing to get . . . . re interesting\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: so ready to take my hair out\n",
            "True answer: then take it out\n",
            "Predicted: i m aware\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: i feel that way about most of my family\n",
            "True answer: the struggle is real\n",
            "Predicted: i m watching that too all of how for dedication until for the too these maybe\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: durham bound today\n",
            "True answer: i need to make my way back to raleigh i miss it up there\n",
            "Predicted: story . <unk> .\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input:  rd question doesn t this indicate u r also corrupt amp helping ruin the democratic party ? potus wall st controlled .\n",
            "True answer: how does sending her random holiday greetings do that ? explain i am curious .\n",
            "Predicted: eh is it is much . everyone s no fear . their our people you prevail you and s that d have new\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: that s a factor keep in mind local dr s office rejects none insured\n",
            "True answer: but i m not talking about uninsured going to er though\n",
            "Predicted: is found beautiful\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: i m free from pm and after pm pct .\n",
            "True answer: if u get on tonight post a card and dm it to me .\n",
            "Predicted: aww . . we was !\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: so what is future for having hella kids by different women ? lmao he aint a hoe ? but ciara is ? ok\n",
            "True answer: who said he wasn t ?\n",
            "Predicted: or really watch it amp he is is\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: hoy tara englishan tayo ! hahahahahaha\n",
            "True answer: sure sure\n",
            "Predicted: sure sure\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: lol it s the motherly instinct in me but frfr get you some rest so you ll feel better\n",
            "True answer: i can see it i guess i ll make a doctors appointment\n",
            "Predicted: oh yes i was my type of it amp ever\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: you not good nigga\n",
            "True answer: lol you heard what i said mf\n",
            "Predicted: pissed\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: always ?\n",
            "True answer: yeah i always gotta be the bad guy i m sick of it stomps the floor and crosses arms \n",
            "Predicted: for you\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: haha what kinda chinese food do you like ?and wats look like in us ?\n",
            "True answer: we have general to chicken . which isn t even sold in china . it s good though .\n",
            "Predicted: pretty much most . shaschwan and hunan . like spice\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: don t rip on him to hard raider brother .\n",
            "True answer: i just don t understand the logic . . . or should i say the lack of logic\n",
            "Predicted: when he one at time\n",
            "Continue? [Y/n]\n",
            "_\n",
            "Input: we hear most about sensitivity training for companies interacting with the general public hospitality healthcare etc . why not all ?\n",
            "True answer: it s not as tho marginalized ppl aren t also consumers or potential hires for companies . everyone should learn to understand inclusivity .\n",
            "Predicted: nope always should be at the draw and support apps themselves and how are you to learn someone for help ?\n",
            "Continue? [Y/n]n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom tweet answer"
      ],
      "metadata": {
        "id": "8nF1BZVz5Dr5"
      },
      "id": "8nF1BZVz5Dr5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U1SMnAbXAQJ"
      },
      "outputs": [],
      "source": [
        "def build_sequences_tweet(tweet, word2idx):\n",
        "    tweet_tokenized = word_tokenize(tweet)\n",
        "    tweet_seq = []\n",
        "    unk_idx = word2idx_eng[unk]\n",
        "    for word in tweet_tokenized:\n",
        "        tweet_seq.append(word2idx_eng.get(word, unk_idx))\n",
        "    \n",
        "    return tweet_seq\n"
      ],
      "id": "1U1SMnAbXAQJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yMovHR4UuUu"
      },
      "outputs": [],
      "source": [
        "def answer_tweet(tweet, mode=\"greedy\"):\n",
        "    tweet_seq = build_sequences_tweet(tweet, word2idx_eng)\n",
        "    tweet_seq = torch.LongTensor(tweet_seq).unsqueeze(1)\n",
        "    return decode_sequence(tweet_seq, mode)"
      ],
      "id": "1yMovHR4UuUu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O_b6UL7aTsnU",
        "outputId": "4028581d-9201-4af3-e082-952a03c0e192"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what sports ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "tweet = \"im going to watch some football tonight and eat chinese\"\n",
        "answer_tweet (tweet, \"sample\")"
      ],
      "id": "O_b6UL7aTsnU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "619ca5cd-3eef-4515-932a-c9b882c22480"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "619ca5cd-3eef-4515-932a-c9b882c22480"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "twitter-chatbot-Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}