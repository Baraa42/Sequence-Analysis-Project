{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27e24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df92808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20_000\n",
    "NUM_SAMPLES = 10_000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f4be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 8490\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "input_texts = []  # sentence in original language\n",
    "target_texts = []  # sentence in target language\n",
    "target_text_inputs = []  #  sentence in target language offset by 1\n",
    "t = 0\n",
    "for line in open(\"./twitter_tab_format.txt\"):\n",
    "    # only keep a limited number of samples\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "    line = line.rstrip()\n",
    "    # input and targets are separeted by tab\n",
    "    if \"\\t\" not in line:\n",
    "        continue\n",
    "\n",
    "    # split up the input and translation\n",
    "    input_text, translation = line.split(\"\\t\")[:2]\n",
    "\n",
    "    # make the target input and output\n",
    "    # recall we'll be using teacher forcing\n",
    "    target_text = translation + \" <eos>\"\n",
    "    target_text_input = \"<sos> \" + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_text_inputs.append(target_text_input)\n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4651a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9908 unique input tokens\n"
     ]
    }
   ],
   "source": [
    "#  tokenize inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "\n",
    "# get word -> integer mapping\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print(\"Found %s unique input tokens\" % len(word2idx_inputs))\n",
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d74c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13053 unique output tokens\n"
     ]
    }
   ],
   "source": [
    "# tokenize outputs\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters=\"\")\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_text_inputs)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_text_inputs)\n",
    "\n",
    "# get word -> integer mapping for outputs\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print(\"Found %s unique output tokens\" % len(word2idx_outputs))\n",
    "\n",
    "\n",
    "# store number of output wors for later\n",
    "# remember to add 1 since indexing start at 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0a3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_data.shape: (8490, 34)\n",
      "encoder_data[0]: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  227   27 5070   51   65\n",
      "    4   34   77   16  278  196]\n",
      "decoder_data.shape: (8490, 33)\n",
      "decoder_data[0]: [   2   18  120  479  380    8   27   82   55 5453   55    3 1441   15\n",
      " 5454   12   67 2243   26   27  166    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(\n",
    "    target_sequences_inputs, maxlen=max_len_target, padding=\"post\"\n",
    ")\n",
    "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
    "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f883a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors ...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print(\"loading word vectors ...\")\n",
    "word2vec_path = '../Lazyprogrammer/large_files/glove.6B/glove.6B.%sd.txt'\n",
    "word2vec = {}\n",
    "with open(\n",
    "    os.path.join(word2vec_path % EMBEDDING_DIM)\n",
    ") as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2]\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.array(values[1:], dtype=\"float32\")\n",
    "        word2vec[word] = vec\n",
    "    print(\"Found %s word vectors.\" % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "909b1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print(\"Filling pre-trained embeddings...\")\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91f26677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data set\n",
    "class TwitterDatset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.encoder_in = torch.from_numpy(encoder_inputs).int()\n",
    "        self.decoder_in = torch.from_numpy(decoder_inputs).int()\n",
    "        self.decoder_out = torch.from_numpy(decoder_targets).float()\n",
    "        #self.decoder_out = torch.from_numpy(decoder_targets_one_hot).float()\n",
    "    def __len__(self):\n",
    "        return len(encoder_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        encoder_in = self.encoder_in[idx]\n",
    "        decoder_in = self.decoder_in[idx]\n",
    "        decoder_out = self.decoder_out[idx]\n",
    "\n",
    "    \n",
    "        return encoder_in, decoder_in, decoder_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00baa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset\n",
    "twitter_dataset = TwitterDatset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911971a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=twitter_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "386f5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an embedding layer\n",
    "# freeze the layer\n",
    "embedding_layer = nn.Embedding(num_words, EMBEDDING_DIM)  # vocab size  # embedding dim\n",
    "embedding_layer.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "embedding_layer.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de69bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "# EMBEDDING_DIM = 100\n",
    "# LATENT_DIM = 256\n",
    "# T encoder = 4\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size # LATENT_DIM\n",
    "        self.num_layers = num_layers # 1 or 2\n",
    "\n",
    "        self.embedding = embedding_layer # vocab size x EMBEDDING_DIM\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True, dropout=p) # -> T x N x 2*hidden\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (N, T encoder) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x)) # (T encoder, N, EMBEDDING_DIM)\n",
    "        # embedding shape: # (T encoder, N, EMBEDDING_DIM)\n",
    "\n",
    "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
    "        #  encoder_states shape:  (T encoder *num_layers , N, 2*hidden_size) \n",
    "        # hidden, cell : (2*num_layers, N, hidden_size) bidirectional = True\n",
    "        \n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        # now : hidden, cell : (num_layers, N, hidden_size) \n",
    "        \n",
    "        return encoder_states, hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42daaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM = 100\n",
    "# LATENT_DIM = 256\n",
    "# T decoder = 11\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size # LATENT_DIM\n",
    "        self.num_layers = num_layers  # 1 or 2\n",
    " \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # input size = vocab fr size = num_words_output\n",
    "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers, dropout=p) \n",
    "        # -> T decoder x N x hidden\n",
    "       \n",
    "        self.energy = nn.Linear(hidden_size*3,1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # ->  T x N x output size\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (N, 1), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0) # -> (1, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        # (sequence_length * num_layers, N, hidden_size)\n",
    "            \n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        # -> (sequence_length, N, 1)\n",
    "        attention = self.softmax(energy) \n",
    "        #(sequence_length, N, 1)\n",
    "        attention = attention.permute(1,2,0)\n",
    "        #(N, 1, sequence_length)\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        #(N, T, 2*hidden)\n",
    "            \n",
    "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        # (N, 1, 2*hidden) -> (1, N, 2*hidden)\n",
    "        \n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        # (1, N, 2*hidden + embedding_size) ->(1, N,  hidden)\n",
    "        # outputs shape: (1, N,  hidden)\n",
    "        # hidden, cell: (1, N,  hidden)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # -> (1, N, output_size)\n",
    "\n",
    "        # predictions shape: (N, 1, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "        # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1c131bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # source = encoder_inputs\n",
    "        # target = encoder_inputs\n",
    "        batch_size = source.shape[1] # source (T_encoder, N)\n",
    "        target_len = target.shape[0] # target (T_decoder, N)\n",
    "        target_vocab_size = num_words_output # check this is correct num_words = len(word2idx_outputs) + 1 \n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device) # (T_decoder, N, vocab)\n",
    "\n",
    "        encoder_states, hidden, cell = self.encoder(source) # (num_layers, N, hidden_size) \n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0] # (1, N)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            #hidden, cell = hidden.squeeze(1), cell.squeeze(1)\n",
    "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96fede43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "target_len: 33\n",
      "target_vocab_size: 13054\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        batch_size = encoder_in.shape[1] # source (T_encoder, N)\n",
    "        target_len = decoder_in.shape[0] # target (T_decoder, N)\n",
    "        target_vocab_size = num_words_output\n",
    "        print(\"batch_size:\", batch_size)\n",
    "        print(\"target_len:\", target_len)\n",
    "        print(\"target_vocab_size:\", target_vocab_size)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53741ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "input_size_encoder = num_words\n",
    "input_size_decoder = num_words_output\n",
    "output_size = num_words_output\n",
    "encoder_embedding_size = EMBEDDING_DIM\n",
    "decoder_embedding_size = EMBEDDING_DIM\n",
    "hidden_size = LATENT_DIM  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eee8a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eeb0e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d049655b-7944-4832-bdc6-51bdc5090fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('./twitter_chatbot.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75ddda6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(9909, 200)\n",
       "    (rnn): LSTM(200, 512, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(13054, 200)\n",
       "    (rnn): LSTM(1224, 512, num_layers=2, dropout=0.5)\n",
       "    (energy): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "    (relu): ReLU()\n",
       "    (fc): Linear(in_features=512, out_features=13054, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#, lr=learning_rate)#, weight_decay=1e-3)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3323154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 200]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 64, 512), got [1, 64, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5256/1365656738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Forward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Output is of shape (batch_size, trg_len, output_dim) but Cross Entropy Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/flight/env/conda+jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5256/1378806820.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Use previous hidden, cell as context from encoder at start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#hidden, cell = hidden.squeeze(1), cell.squeeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Store next output prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/flight/env/conda+jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5256/520945056.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_states, hidden, cell)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# rnn_input: (1, N, hidden_size*2 + embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;31m# (1, N, 2*hidden + embedding_size) ->(1, N,  hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# outputs shape: (1, N,  hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/flight/env/conda+jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apps/flight/env/conda+jupyter/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m/opt/apps/flight/env/conda+jupyter/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    631\u001b[0m                            ):\n\u001b[1;32m    632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0m\u001b[1;32m    634\u001b[0m                                'Expected hidden[0] size {}, got {}')\n\u001b[1;32m    635\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n",
      "\u001b[0;32m/opt/apps/flight/env/conda+jupyter/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    224\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 64, 512), got [1, 64, 512]"
     ]
    }
   ],
   "source": [
    "sentence = \"I love dogs\"\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "train_losses = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    #checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    #save_checkpoint(checkpoint)\n",
    "\n",
    "    #model.eval()\n",
    "\n",
    "    \n",
    "    #translated_sentence = translate_sentence(sentence)\n",
    "\n",
    "    #print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        #inp_data = batch.src.to(device)\n",
    "        #target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(encoder_in, decoder_in)\n",
    "\n",
    "        # Output is of shape (batch_size, trg_len, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have batch_size * output_words that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        #output = output[1:].reshape(-1, output.shape[2])\n",
    "        #target = target[1:].reshape(-1)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target.long())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    epoch_loss = np.mean(train_loss)           \n",
    "    train_losses[epoch] = epoch_loss\n",
    "    print(f'Loss: {epoch_loss:.4f}')\n",
    "        # Plot to tensorboard\n",
    "        #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        #step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2280fd65-c6bd-4fa0-adf1-96c4ffaa1efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some plots\n",
    "plt.plot(train_losses, label=\"train_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc144df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './twitter_chatbot.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ace5940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map indexes back into real words\n",
    "# so wwe can view the results\n",
    "idx2word_eng = {v: w for w, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v: w for w, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f52db17d-e806-41cf-99a2-dca07f1ae166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # encode the input as state vectors.\n",
    "    input_seq = input_seq.to(device)\n",
    "    with torch.no_grad():\n",
    "        encoder_states, h, c = encoder_net(input_seq)\n",
    "\n",
    "        # generate empty target seq of length 1\n",
    "        target_seq = torch.zeros(1).int().to(device)\n",
    "\n",
    "        # populate the first character of target sequence with the start character\n",
    "        # NOTE: tokenizer lower cases all words\n",
    "        target_seq[0] = word2idx_outputs[\"<sos>\"]\n",
    "\n",
    "        # if we get this we break\n",
    "        eos = word2idx_outputs[\"<eos>\"]\n",
    "\n",
    "        # create translation\n",
    "        output_sentence = []\n",
    "        for _ in range(max_len_target):\n",
    "            output_tokens, h, c = decoder_net(target_seq, encoder_states, h, c)\n",
    "\n",
    "            # get next word\n",
    "            idx = output_tokens.argmax(1).item()\n",
    "\n",
    "            # end of sentence EOS\n",
    "            if eos == idx:\n",
    "                break\n",
    "\n",
    "            word = \"\"\n",
    "            if idx > 0:\n",
    "                word = idx2word_trans[idx]\n",
    "                output_sentence.append(word)\n",
    "\n",
    "            # update the decoder input\n",
    "            # which is just the word just generated\n",
    "            target_seq[0] = idx\n",
    "            #states_value = [h, c]\n",
    "\n",
    "        return \" \".join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d55bba23-35e4-4eb0-a2d4-57c388d17104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: warriors big are soft. warriors gotta fill the center position\n",
      "Predicted: i\n",
      "True answer: that's why trade klay thompson for demarcus cousins <eos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: time to watch #scream season 2\n",
      "Predicted: i\n",
      "True answer: you'll be sooooo freaking on the edge of your seat <eos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: interesting. ive had so many issues with those apps before that ive given them all up\n",
      "Predicted: i coughing\n",
      "True answer: i would never install an app for something this stupid ... but my options were really small text or blurry text on scaled. <eos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: i use it primarily to communicate with co-workers.  never thought of it as a cs solution.  interesting idea.\n",
      "Predicted: i coughing\n",
      "True answer: i love it for co-worker communication. always wondered what a support channel might look like. <eos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: help seriously\n",
      "Predicted: i coughing\n",
      "True answer: 'smatter hon? <eos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: so hard it's hurting my cheeks\n",
      "Predicted: i coughing\n",
      "True answer: my lil jasmine is the cutest thing ever <eos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: sorry for busy with my work.its 1606 in china,same district with shanghai\n",
      "Predicted: i\n",
      "True answer: so dinner time. wish i could eat real chinese food. not what get in the us <eos\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # do some translation\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i : i + 1]\n",
    "    input_seq = torch.from_numpy(input_seq).to(device).permute(1,0)\n",
    "    pred_answer = decode_sequence(input_seq)\n",
    "    true_answer = target_texts[i]\n",
    "    print(\"_\")\n",
    "    print(\"Input:\", input_texts[i])\n",
    "    print(\"Predicted:\", pred_answer)\n",
    "    print(\"True answer:\", true_answer[:-1])\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith(\"n\"):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc7ee253-758a-4e54-a12c-4384eea0e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence=\"I love dogs\"):\n",
    "    sentence = [sentence]\n",
    "    sequence = tokenizer_inputs.texts_to_sequences(sentence)\n",
    "    sequence = torch.Tensor(sequence).int()\n",
    "    translation = decode_sequence(sequence)\n",
    "    print(translation)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20276f79-a701-4dfe-9d02-10cebc0d9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging stuff\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        encoder_states, hidden, cell = encoder_net(encoder_in)\n",
    "        \n",
    "        x = decoder_in[0]\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = decoder_net.dropout(decoder_net.embedding(x))\n",
    "        print(x.shape)\n",
    "        print(\"embedding shape:\", embedding.shape)\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        print(\"sequence_length:\", sequence_length)\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        print(\"h_reshaped.shape:\", h_reshaped.shape)\n",
    "        energy = decoder_net.relu(decoder_net.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        print(\"energy.shape:\", energy.shape)\n",
    "        attention = decoder_net.softmax(energy) \n",
    "        attention = attention.permute(1,2,0)\n",
    "        print(\"attention.shape:\", attention.shape)\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        print(\"encoder_states.shape:\", encoder_states.shape)\n",
    "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        print(\"context_vector shape:\",context_vector.shape)\n",
    "        print(\"embedding shape:\", embedding.shape)\n",
    "        #predictions, hidden, cell = decoder_net(x, encoder_states, hidden, cell)\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        print(\"rnn_input.shape:\",rnn_input.shape)\n",
    "        outputs, (hidden, cell) = decoder_net.rnn(rnn_input, (hidden, cell))\n",
    "        print(\"outputs.shape:\",outputs.shape )\n",
    "        print(\"hidden.shape:\",hidden.shape )\n",
    "        print(\"cell.shape:\",cell.shape )\n",
    "        predictions = decoder_net.fc(outputs).squeeze(0)\n",
    "        print(\"predictions.shape:\",predictions.shape )\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fe001d2-69ac-4805-8ec1-ddc84ec598e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'and then that mouse had the nerve to try to eat our kibble!  let this be a lesson fur all the other mousies! <eos>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ca5cd-3eef-4515-932a-c9b882c22480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
