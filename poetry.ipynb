{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22304af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torchtext.data as ttd\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b0f51d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/baraa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# loading stop words\n",
    "nltk.load('english', format='text');\n",
    "nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01c9459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_VOCAB_SIZE = 3_000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2_000\n",
    "LATENT_DIM = 25\n",
    "\n",
    "SOS = \"<sos>\"\n",
    "EOS = \"<eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ef2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "robert_frost = '../../Lazyprogrammer/seq2seq/robert_frost.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15dd26ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines : 1436\n"
     ]
    }
   ],
   "source": [
    "# load in the data \n",
    "lines = []\n",
    "for line in open(robert_frost):\n",
    "    line = line.lower().rstrip()\n",
    "    if not line:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "        \n",
    "\n",
    "\n",
    "print(\"number of lines :\", len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5dc7335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and set off briskly for so slow a thing,\n"
     ]
    }
   ],
   "source": [
    "# looking at some random lines\n",
    "idx = np.random.randint(len(lines))\n",
    "print(lines[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1674790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "voc = []\n",
    "l = []\n",
    "#stop_words = stopwords.words(\"english\")\n",
    "\n",
    "for line in open(robert_frost):\n",
    "    line = word_tokenize(re.sub(\"\\W+\", \" \", line.lower())) \n",
    "    for w in line:\n",
    "        if w not in voc: # and w not in stop_words:\n",
    "            voc.append(w)\n",
    "\n",
    "voc = voc + [EOS, SOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cf10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocab : 2120 tokens\n"
     ]
    }
   ],
   "source": [
    "# convert vocabulary to indices and keep the order (OrderedDict)\n",
    "word2idx = {}\n",
    "for idx, w in enumerate(reversed(voc)):\n",
    "    word2idx[w] = idx + 1 # key=word, item=index\n",
    "\n",
    "word2idx_keys = word2idx.keys()\n",
    "print(\"Length of vocab : {0:d} tokens\".format(len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662a99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing inputs and targets\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "for line in lines:\n",
    "    line = word_tokenize(re.sub(\"\\W+\", \" \", line.lower())) \n",
    "    input_sequence = [word2idx[SOS]] + [word2idx[w] for w in line]\n",
    "    target_sequence = [word2idx[w] for w in line] + [word2idx[EOS]]\n",
    "    input_sequences.append(input_sequence)\n",
    "    target_sequences.append(target_sequence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f2a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sequence_length: 15\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = max(len(s) for s in input_sequences)\n",
    "print(\"max_sequence_length:\", max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c53062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post padding\n",
    "k = 0\n",
    "for i in range(len(input_sequences)):\n",
    "    if len(input_sequences[i]) < max_sequence_length:\n",
    "        input_sequences[i] += (max_sequence_length - len(input_sequences[i])) * [0]\n",
    "        target_sequences[i] += (max_sequence_length - len(target_sequences[i])) * [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2aa2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1436, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of data\n",
    "len(input_sequences), len(input_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d83b1520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors ...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors \n",
    "# can download here \n",
    "# word2vec = torchtext.vocab.GloVe(name=\"6B\", dim=EMBEDDING_DIM) \n",
    "print(\"loading word vectors ...\")\n",
    "word2vec_path = '../../Lazyprogrammer/large_files/glove.6B/glove.6B.%sd.txt'\n",
    "word2vec = {}\n",
    "with open(\n",
    "    os.path.join(word2vec_path % EMBEDDING_DIM)\n",
    ") as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2]\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.array(values[1:], dtype=\"float32\")\n",
    "        word2vec[word] = vec\n",
    "    print(\"Found %s word vectors.\" % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b935170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print(\"Filling pre-trained embeddings...\")\n",
    "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04e557c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot the targets (can't use sparse categorical cross entropy)\n",
    "one_hot_targets = np.zeros((len(input_sequences), max_sequence_length, num_words))\n",
    "for i, target_sequence in enumerate(target_sequences):\n",
    "    for t, word in enumerate(target_sequence):\n",
    "        if word > 0:\n",
    "            one_hot_targets[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4b175a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an embedding layer\n",
    "# freeze the layer\n",
    "embedding_layer = nn.Embedding(num_words, EMBEDDING_DIM,)  # vocab size  # embedding dim\n",
    "embedding_layer.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "embedding_layer.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56e6cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
