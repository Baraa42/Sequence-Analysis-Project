{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b27e24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df92808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20_000\n",
    "NUM_SAMPLES = 10_000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f4be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# load in the data\n",
    "input_texts = []  # sentence in original language\n",
    "target_texts = []  # sentence in target language\n",
    "target_text_inputs = []  #  sentence in target language offset by 1\n",
    "t = 0\n",
    "for line in open(\"./fra.txt\"):\n",
    "    # only keep a limited number of samples\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "    line = line.rstrip()\n",
    "    # input and targets are separeted by tab\n",
    "    if \"\\t\" not in line:\n",
    "        continue\n",
    "\n",
    "    # split up the input and translation\n",
    "    input_text, translation = line.split(\"\\t\")[:2]\n",
    "\n",
    "    # make the target input and output\n",
    "    # recall we'll be using teacher forcing\n",
    "    target_text = translation + \" <eos>\"\n",
    "    target_text_input = \"<sos> \" + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_text_inputs.append(target_text_input)\n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4651a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2033 unique input tokens\n"
     ]
    }
   ],
   "source": [
    "#  tokenize inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "\n",
    "# get word -> integer mapping\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print(\"Found %s unique input tokens\" % len(word2idx_inputs))\n",
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d74c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5680 unique output tokens\n"
     ]
    }
   ],
   "source": [
    "# tokenize outputs\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters=\"\")\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_text_inputs)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_text_inputs)\n",
    "\n",
    "# get word -> integer mapping for outputs\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print(\"Found %s unique output tokens\" % len(word2idx_outputs))\n",
    "\n",
    "\n",
    "# store number of output wors for later\n",
    "# remember to add 1 since indexing start at 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0a3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_data.shape: (10000, 4)\n",
      "encoder_data[0]: [0 0 0 9]\n",
      "decoder_data.shape: (10000, 11)\n",
      "decoder_data[0]: [ 2 43  4  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(\n",
    "    target_sequences_inputs, maxlen=max_len_target, padding=\"post\"\n",
    ")\n",
    "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
    "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f883a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word vectors ...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load in pre-trained word vectors\n",
    "print(\"loading word vectors ...\")\n",
    "word2vec_path = '../Lazyprogrammer/large_files/glove.6B/glove.6B.%sd.txt'\n",
    "word2vec = {}\n",
    "with open(\n",
    "    os.path.join(word2vec_path % EMBEDDING_DIM)\n",
    ") as f:\n",
    "    # is just a space-separated text file in the format:\n",
    "    # word vec[0] vec[1] vec[2]\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.array(values[1:], dtype=\"float32\")\n",
    "        word2vec[word] = vec\n",
    "    print(\"Found %s word vectors.\" % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909b1ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print(\"Filling pre-trained embeddings...\")\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f26677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data set\n",
    "class Translation(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.encoder_in = torch.from_numpy(encoder_inputs).int()\n",
    "        self.decoder_in = torch.from_numpy(decoder_inputs).int()\n",
    "        self.decoder_out = torch.from_numpy(decoder_targets).float()\n",
    "        #self.decoder_out = torch.from_numpy(decoder_targets_one_hot).float()\n",
    "    def __len__(self):\n",
    "        return len(encoder_inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        encoder_in = self.encoder_in[idx]\n",
    "        decoder_in = self.decoder_in[idx]\n",
    "        decoder_out = self.decoder_out[idx]\n",
    "\n",
    "    \n",
    "        return encoder_in, decoder_in, decoder_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00baa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset\n",
    "translation_dataset = Translation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "911971a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=translation_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386f5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an embedding layer\n",
    "# freeze the layer\n",
    "embedding_layer = nn.Embedding(num_words, EMBEDDING_DIM)  # vocab size  # embedding dim\n",
    "embedding_layer.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
    "embedding_layer.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de69bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some configuration\n",
    "# EMBEDDING_DIM = 100\n",
    "# LATENT_DIM = 256\n",
    "# T encoder = 4\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size,embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size # LATENT_DIM\n",
    "        self.num_layers = num_layers # 1 or 2\n",
    "\n",
    "        self.embedding = embedding_layer # vocab size x EMBEDDING_DIM\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)#, dropout=p) # -> T x N x 2*hidden\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size*2, hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (N, T encoder) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x)) # (T encoder, N, EMBEDDING_DIM)\n",
    "        # embedding shape: # (T encoder, N, EMBEDDING_DIM)\n",
    "\n",
    "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
    "        #  encoder_states shape:  (T encoder *num_layers , N, 2*hidden_size) \n",
    "        # hidden, cell : (2*num_layers, N, hidden_size) bidirectional = True\n",
    "        \n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        # now : hidden, cell : (num_layers, N, hidden_size) \n",
    "        \n",
    "        return encoder_states, hidden, cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42daaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING_DIM = 100\n",
    "# LATENT_DIM = 256\n",
    "# T decoder = 11\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size # LATENT_DIM\n",
    "        self.num_layers = num_layers  # 1 or 2\n",
    " \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # input size = vocab fr size = num_words_output\n",
    "        self.rnn = nn.LSTM(hidden_size*2 + embedding_size, hidden_size, num_layers)#, dropout=p) \n",
    "        # -> T decoder x N x hidden\n",
    "       \n",
    "        self.energy = nn.Linear(hidden_size*3,1)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # ->  T x N x output size\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, encoder_states, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (N, 1), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0) # -> (1, N)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        # (sequence_length * num_layers, N, hidden_size)\n",
    "            \n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        # -> (sequence_length, N, 1)\n",
    "        attention = self.softmax(energy) \n",
    "        #(sequence_length, N, 1)\n",
    "        attention = attention.permute(1,2,0)\n",
    "        #(N, 1, sequence_length)\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        #(N, T, 2*hidden)\n",
    "            \n",
    "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        # (N, 1, 2*hidden) -> (1, N, 2*hidden)\n",
    "        \n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        # (1, N, 2*hidden + embedding_size) ->(1, N,  hidden)\n",
    "        # outputs shape: (1, N,  hidden)\n",
    "        # hidden, cell: (1, N,  hidden)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # -> (1, N, output_size)\n",
    "\n",
    "        # predictions shape: (N, 1, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "        # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1c131bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        # source = encoder_inputs\n",
    "        # target = encoder_inputs\n",
    "        batch_size = source.shape[1] # source (T_encoder, N)\n",
    "        target_len = target.shape[0] # target (T_decoder, N)\n",
    "        target_vocab_size = num_words_output # check this is correct num_words = len(word2idx_outputs) + 1 \n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device) # (T_decoder, N, vocab)\n",
    "\n",
    "        encoder_states, hidden, cell = self.encoder(source) # (num_layers, N, hidden_size) \n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0] # (1, N)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            #hidden, cell = hidden.squeeze(1), cell.squeeze(1)\n",
    "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96fede43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "target_len: 11\n",
      "target_vocab_size: 5681\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        batch_size = encoder_in.shape[1] # source (T_encoder, N)\n",
    "        target_len = decoder_in.shape[0] # target (T_decoder, N)\n",
    "        target_vocab_size = num_words_output\n",
    "        print(\"batch_size:\", batch_size)\n",
    "        print(\"target_len:\", target_len)\n",
    "        print(\"target_vocab_size:\", target_vocab_size)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53741ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "input_size_encoder = num_words\n",
    "input_size_decoder = num_words_output\n",
    "output_size = num_words_output\n",
    "encoder_embedding_size = EMBEDDING_DIM\n",
    "decoder_embedding_size = EMBEDDING_DIM\n",
    "hidden_size = LATENT_DIM  # Needs to be the same for both RNN's\n",
    "num_layers = 1\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eee8a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb0e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d049655b-7944-4832-bdc6-51bdc5090fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('./translation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75ddda6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(2034, 100)\n",
       "    (rnn): LSTM(100, 256, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc_cell): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(5681, 100)\n",
       "    (rnn): LSTM(612, 256)\n",
       "    (energy): Linear(in_features=768, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "    (relu): ReLU()\n",
       "    (fc): Linear(in_features=256, out_features=5681, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#, lr=learning_rate, weight_decay=1e-3)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3323154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 25]\n",
      "Loss: 1.1982\n",
      "[Epoch 1 / 25]\n",
      "Loss: 0.9891\n",
      "[Epoch 2 / 25]\n",
      "Loss: 1.2680\n",
      "[Epoch 3 / 25]\n",
      "Loss: 1.2684\n",
      "[Epoch 4 / 25]\n",
      "Loss: 1.2032\n",
      "[Epoch 5 / 25]\n",
      "Loss: 1.2618\n",
      "[Epoch 6 / 25]\n",
      "Loss: 1.1424\n",
      "[Epoch 7 / 25]\n",
      "Loss: 1.1279\n",
      "[Epoch 8 / 25]\n",
      "Loss: 1.1884\n",
      "[Epoch 9 / 25]\n",
      "Loss: 1.0906\n",
      "[Epoch 10 / 25]\n",
      "Loss: 1.1706\n",
      "[Epoch 11 / 25]\n",
      "Loss: 1.0638\n",
      "[Epoch 12 / 25]\n",
      "Loss: 1.1611\n",
      "[Epoch 13 / 25]\n",
      "Loss: 1.1256\n",
      "[Epoch 14 / 25]\n",
      "Loss: 1.0099\n",
      "[Epoch 15 / 25]\n",
      "Loss: 1.2698\n",
      "[Epoch 16 / 25]\n",
      "Loss: 1.0057\n",
      "[Epoch 17 / 25]\n",
      "Loss: 1.0452\n",
      "[Epoch 18 / 25]\n",
      "Loss: 0.9293\n",
      "[Epoch 19 / 25]\n",
      "Loss: 0.9947\n",
      "[Epoch 20 / 25]\n",
      "Loss: 1.0754\n",
      "[Epoch 21 / 25]\n",
      "Loss: 1.0087\n",
      "[Epoch 22 / 25]\n",
      "Loss: 1.0163\n",
      "[Epoch 23 / 25]\n",
      "Loss: 0.9838\n",
      "[Epoch 24 / 25]\n",
      "Loss: 1.0834\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love dogs\"\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "train_losses = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    #checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    #save_checkpoint(checkpoint)\n",
    "\n",
    "    #model.eval()\n",
    "\n",
    "    \n",
    "    #translated_sentence = translate_sentence(sentence)\n",
    "\n",
    "    #print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    #model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        #inp_data = batch.src.to(device)\n",
    "        #target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(encoder_in, decoder_in)\n",
    "\n",
    "        # Output is of shape (batch_size, trg_len, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have batch_size * output_words that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        #output = output[1:].reshape(-1, output.shape[2])\n",
    "        #target = target[1:].reshape(-1)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target.long())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    epoch_loss = np.mean(train_loss)           \n",
    "    train_losses[epoch] = epoch_loss\n",
    "    print(f'Loss: {epoch_loss:.4f}')\n",
    "        # Plot to tensorboard\n",
    "        #writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        #step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2280fd65-c6bd-4fa0-adf1-96c4ffaa1efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some plots\n",
    "plt.plot(train_losses, label=\"train_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc144df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './translation_attention.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ace5940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# map indexes back into real words\n",
    "# so wwe can view the results\n",
    "idx2word_eng = {v: w for w, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v: w for w, v in word2idx_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f52db17d-e806-41cf-99a2-dca07f1ae166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # encode the input as state vectors.\n",
    "    input_seq = input_seq.to(device)\n",
    "    with torch.no_grad():\n",
    "        encoder_states, h, c = encoder_net(input_seq)\n",
    "\n",
    "        # generate empty target seq of length 1\n",
    "        target_seq = torch.zeros(1).int().to(device)\n",
    "\n",
    "        # populate the first character of target sequence with the start character\n",
    "        # NOTE: tokenizer lower cases all words\n",
    "        target_seq[0] = word2idx_outputs[\"<sos>\"]\n",
    "\n",
    "        # if we get this we break\n",
    "        eos = word2idx_outputs[\"<eos>\"]\n",
    "\n",
    "        # create translation\n",
    "        output_sentence = []\n",
    "        for _ in range(max_len_target):\n",
    "            output_tokens, h, c = decoder_net(target_seq, encoder_states, h, c)\n",
    "\n",
    "            # get next word\n",
    "            idx = output_tokens.argmax(1).item()\n",
    "\n",
    "            # end of sentence EOS\n",
    "            if eos == idx:\n",
    "                break\n",
    "\n",
    "            word = \"\"\n",
    "            if idx > 0:\n",
    "                word = idx2word_trans[idx]\n",
    "                output_sentence.append(word)\n",
    "\n",
    "            # update the decoder input\n",
    "            # which is just the word just generated\n",
    "            target_seq[0] = idx\n",
    "            #states_value = [h, c]\n",
    "\n",
    "        return \" \".join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d55bba23-35e4-4eb0-a2d4-57c388d17104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I'm retired.\n",
      "Translation: suis retraité.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: They hugged.\n",
      "Translation: se sont embrassés.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I'm not mean.\n",
      "Translation: ne suis pas méchant.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Stop reading.\n",
      "Translation: de lire.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Tom drowned.\n",
      "Translation: se noyé.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Be careful now.\n",
      "Translation: attention maintenant.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Sit down!\n",
      "Translation: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I'm stubborn.\n",
      "Translation: suis têtu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Go to sleep.\n",
      "Translation: \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Turn right.\n",
      "Translation: à droite.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I am sure.\n",
      "Translation: suis certain.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Tom's mean.\n",
      "Translation: est méchant.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: You're drunk.\n",
      "Translation: êtes saoul.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I'm the coach.\n",
      "Translation: suis l'entraîneur.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I stood.\n",
      "Translation: me suis tenu\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I cry a lot.\n",
      "Translation: pleure beaucoup.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: I use it.\n",
      "Translation: l'utilise. usage.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: We can try it.\n",
      "Translation: pouvons l'essayer.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: You're a pig.\n",
      "Translation: es un égoïste.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: He drives fast.\n",
      "Translation: court vite.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Hug me.\n",
      "Translation: dans tes bras\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: You're sleepy.\n",
      "Translation: êtes endormie.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Come on, Tom.\n",
      "Translation: tom.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: It's me!\n",
      "Translation: bibi !\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "Input: Ignore that.\n",
      "Translation: ça.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? [Y/n] n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # do some translation\n",
    "    i = np.random.choice(len(input_texts))\n",
    "    input_seq = encoder_inputs[i : i + 1]\n",
    "    input_seq = torch.from_numpy(input_seq).to(device).permute(1,0)\n",
    "    translation = decode_sequence(input_seq)\n",
    "    print(\"_\")\n",
    "    print(\"Input:\", input_texts[i])\n",
    "    print(\"Translation:\", translation)\n",
    "\n",
    "    ans = input(\"Continue? [Y/n]\")\n",
    "    if ans and ans.lower().startswith(\"n\"):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc7ee253-758a-4e54-a12c-4384eea0e6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence=\"I love dogs\"):\n",
    "    sentence = [sentence]\n",
    "    sequence = tokenizer_inputs.texts_to_sequences(sentence)\n",
    "    sequence = torch.Tensor(sequence).int()\n",
    "    translation = decode_sequence(sequence)\n",
    "    print(translation)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20276f79-a701-4dfe-9d02-10cebc0d9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging stuff\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "        train_loss = []\n",
    "        # Get input and targets and get to cuda\n",
    "        # encoder_in, decoder_in, decoder_out\n",
    "        encoder_in, decoder_in, target = batch\n",
    "        encoder_in, decoder_in, target = encoder_in.to(device), decoder_in.to(device), target.to(device)\n",
    "        encoder_in, decoder_in, target = encoder_in.permute(1,0), decoder_in.permute(1,0), target.permute(1,0)\n",
    "        encoder_states, hidden, cell = encoder_net(encoder_in)\n",
    "        \n",
    "        x = decoder_in[0]\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = decoder_net.dropout(decoder_net.embedding(x))\n",
    "        print(x.shape)\n",
    "        print(\"embedding shape:\", embedding.shape)\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        print(\"sequence_length:\", sequence_length)\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        print(\"h_reshaped.shape:\", h_reshaped.shape)\n",
    "        energy = decoder_net.relu(decoder_net.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        print(\"energy.shape:\", energy.shape)\n",
    "        attention = decoder_net.softmax(energy) \n",
    "        attention = attention.permute(1,2,0)\n",
    "        print(\"attention.shape:\", attention.shape)\n",
    "        encoder_states = encoder_states.permute(1,0,2)\n",
    "        print(\"encoder_states.shape:\", encoder_states.shape)\n",
    "        context_vector = torch.bmm(attention, encoder_states).permute(1,0,2)\n",
    "        print(\"context_vector shape:\",context_vector.shape)\n",
    "        print(\"embedding shape:\", embedding.shape)\n",
    "        #predictions, hidden, cell = decoder_net(x, encoder_states, hidden, cell)\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        print(\"rnn_input.shape:\",rnn_input.shape)\n",
    "        outputs, (hidden, cell) = decoder_net.rnn(rnn_input, (hidden, cell))\n",
    "        print(\"outputs.shape:\",outputs.shape )\n",
    "        print(\"hidden.shape:\",hidden.shape )\n",
    "        print(\"cell.shape:\",cell.shape )\n",
    "        predictions = decoder_net.fc(outputs).squeeze(0)\n",
    "        print(\"predictions.shape:\",predictions.shape )\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
